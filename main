import numpy as np
import d3rlpy as d3
import gym
from RLModels.DiscreteRL import DiscreteModel
from SeqEncoders.encoder import encodeTrajectories
from Clustering.XMeans import XMeans
from Embedders.trainExpPolicy import trainExpPolicies
from Clustering.ClusterAttribution import generateClusterAttribution


if __name__ == "__main__":
    # Example usage:
    # 1. Create an instance of the OfflineRLTrainer class
    offline_data = DiscreteModel(env_name="Seaquest-v4", dataset_path="Seaquest-v4")

    # Hardcoded offline data with two trajectories
    offline_data = [
        [  # First trajectory
            ([0.1, 0.2, 0.3], [1, 0], 0.5),
            ([0.4, 0.5, 0.6], [0, 1], 0.6),
            ([0.7, 0.8, 0.9], [1, 0], 0.7)
        ],
        [  # Second trajectory
            ([0.9, 0.8, 0.7], [0, 1], 0.4),
            ([0.6, 0.5, 0.4], [1, 0], 0.3),
            ([0.3, 0.2, 0.1], [0, 1], 0.2)
        ]
    ]

    # 2. Encode the trajectories using encodeTrajectories function
    trajectory_embeddings = encodeTrajectories(offline_data)

    # 3. Cluster the trajectories using XMeans.
    XMeans = XMeans()
    clusters = XMeans.clusterTrajectories(trajectory_embeddings)

    # 4 and 5. Generate data embeddings using generateDataEmbedding function Train the offline RL algorithm using trainOfflineRL function

    # Placeholder function for offlineRLAlgo
    def offlineRLAlgo(complementary_data):
        # Return a dummy policy (could be a more complex object or model in practice)
        return {'policy': 'dummy_policy'}

    # Fake offline data with two trajectories
    offline_data = [
        [([0.1, 0.2], [1], 0.5), ([0.2, 0.3], [0], 0.6)],  # First trajectory
        [([0.3, 0.4], [1], 0.4), ([0.4, 0.5], [0], 0.7)]   # Second trajectory
    ]

    # Fake trajectory embeddings
    # Assuming we have an embedding of size 3 for each trajectory
    trajectory_embeddings = [np.random.rand(3) for _ in offline_data]

    # Fake clusters of trajectories
    clusters = [[0], [1]]

    # Now we call the trainExpPolicies function with the fake data
    explanation_policies, data_embeddings = trainExpPolicies(
        offline_data, trajectory_embeddings, clusters, offlineRLAlgo)


    # Perform cluster attribution for a specific state using generateClusterAttribution function
    state = np.array([0.1, 0.2, 0.3]) # Random state

    def mock_policy(state):
    # Return a dummy action that is the same shape as the state
        return state * 0.5  # Simple transformation for the sake of example
    
    
    original_policy=mock_policy
    original_data_embedding = np.array([0.5, 0.5, 0.5])
    explanation_policies = [mock_policy, mock_policy, mock_policy]

    chosen_cluster = generateClusterAttribution(
        state, original_policy, explanation_policies, original_data_embedding, data_embeddings)

    print("Attributed cluster:", chosen_cluster)


